competition : CommonLit - Evaluate Student Summaries
name : deberta-v3-base
version : CommonLitModelV1
seed : 42
wandb : false
device : 0


#===========================
# platform
#===========================

platform : 
  isgoogle : false
  google : 
    dpath : /content/gdrive/MyDrive/tmpdata/
    opath : /content/gdrive/MyDrive/output/
  featurize : 
    dpath : /home/featurize/data/
    opath : /home/featurize/work/ka-CESS-COMP/work/output/


#===========================
# data process
# 
# dataset['pooling_name']
# args.model['pooling_params']['pooling_name']
#===========================


data:
  prepare:
    prompt_max_len: 512
    add_question: true
    experiment: false
    experiment_rate: 0.5
  dataset:
    pool_question: true
    target_cols: ['content', 'wording']
    text_max_len : 512
    prompt_text_max_len: 1024


#===========================
# model
# params:act[relu6,sigmoid]
#===========================
selected_folds: [0,1,2,3]

model:
  loss: CommonLitLoss
  loss_reduction: mean
  sub_loss: RMSELoss
  use_weights: true
  sub_loss_param:
    reduction: none
  max_len: 1024
  max_len_eval: 1024
  params:
    model_name: microsoft/deberta-v3-base
    span_pool: false
    add_prompt: false
    gradient_checkpointing: true
    multilfc: false
    multilpool: false
    activation: none
    freezing: 4
    REINIT_LAYERS: 0
    init_head: true
    config_path:
  CrosenEoderPara:
    mask_flag : false
    numcross : 3
    numconv1: 0
    d_model : 762
    attParameter : {}
    downConvPara : {}
  CrosAttPara:
    mask_flag : false
    d_model: 762
    d_ff: 762
    n_heads: 8
    factor: 20
    dropout: 0.1
    attType: prob
    activation: relu
  CrosConvPara:
    c_in: 762
    d_model: 762
    kernel_size: 5
  pooling_params :
    pooling_name: MeanPooling
    params : {}
  spans_pooling_params :
    pooling_name : AttentionHead
    params : {}
  spans: ''
  target_cols : ['content', 'wording']
  target_weights :
    avg : [0.5, 0.5]
    wtd : [0.55, 0.45]

#===========================
# optimizer
# LLDR : <1. if(LLDR=1) means no lldr
#===========================
# torch adamw
# optim.AdamW
#===========================
# Debiasing Omission In BertADAM
# Adamw
#===========================


optimizer:
  name : AdamW
  LLDR: 0.9
  HeadLR: 0.0001
  params : 
    lr: 0.00002
    betas: [0.9, 0.999]
    eps: 0.000001
    weight_decay: 0.01
  tparams : 
    lr: 0.00002
    eps: 0.000001
    weight_decay: 0.01
    correct_bias: true

#===========================
# scheduler
# poly,linear,cosine,OneCycleLR
#===========================


scheduler:
  name: cosine
  params : 
    lr_end: 0.0000007
    power: 3
  warmup: 0.1

#===========================
# train_loader
#===========================


train_loader:
  batch_size: 1
  drop_last: true
  num_workers: 4
  pin_memory: true
  shuffle: true

#===========================
# val_loader
#===========================


val_loader:
  batch_size: 1
  drop_last: false
  num_workers: 4
  pin_memory: true
  shuffle: false

test_loader:
  batch_size: 32
  drop_last: false
  num_workers: 4
  pin_memory: true
  shuffle: false
#===========================
# trainer
# max_norm : 
#   init : 1000 
#   experiment :10*batch_size 
#===========================


trainer:
  use_amp: true
  epochs: 4
  sample: true
  gradient_checkpointing: true
  grad_clip: true
  max_norm: 1000
  accum_iter: 0


#===========================
# callbacks
#===========================


callbacks:
  stepwise: true
  save : true
  es: false
  steploss : 
    tqdm : true
    nprint : 3 
  patience: 0
  verbose_eval: 1
  epoch_pct_eval: 0.1
  epoch_eval_dist: uniforme
  metric_track: val_loss
  mode: min
  top_k: 1
  start_eval_epoch : 0
