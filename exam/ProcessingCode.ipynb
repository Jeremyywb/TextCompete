{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa4a8a9",
   "metadata": {},
   "source": [
    "## Take Home Test: Reformat a Public Dataset for LLM Training\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal of this task is to prepare public datasets for more effective use in training and fine-tuning Large Language Models (LLMs). You are required to reformat a specific subset of a public dataset into a structured, consistent format to facilitate its usability.\n",
    "\n",
    "### Detailed Instructions\n",
    "\n",
    "#### 1. Dataset Selection and Preparation\n",
    "\n",
    "- **Dataset:** You are assigned the `Headline` subset of the [AdaptLLM/finance-tasks](https://huggingface.co/datasets/AdaptLLM/finance-tasks) dataset.\n",
    "\n",
    "- **Task Description:** Each entry in the `input` column contains multiple \"Yes\" or \"No\" questions alongside their respective answers. Your task is to:\n",
    "\n",
    "  - Develop a Python script to parse and separate each question and its answer from the entry.\n",
    "  - Save each question-answer pair in a structured JSON format as follows:\n",
    "    ```json\n",
    "    {\n",
    "      \"id\": \"<unique_identifier>\",\n",
    "      \"Question\": \"<question_text>\",\n",
    "      \"Answer\": \"<answer_text>\"\n",
    "    }\n",
    "    ```\n",
    "\n",
    "  - You are encouraged to introduce additional attributes if needed to preserve the integrity and completeness of the information. Adding relevant tag information is strongly recommended.\n",
    "- **Automation Requirement:** The task must be completed using Python. Manual editing or data manipulation is strictly prohibited. Your script should efficiently handle variations in data format within the column.\n",
    "\n",
    "#### 2. Deliverables\n",
    "\n",
    "- **Reformatted Dataset:** Provide the schema of the final format you adopted for saving the results.\n",
    "- **Transformation Code:** Submit the complete code used for converting the dataset into the designated format.\n",
    "- **Statistics:** Report the total number of question-answer pairs extracted from the dataset.\n",
    "- **Performance Metrics:** Document the time taken to complete the dataset cleanup and transformation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28cd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Load\n",
    "# ========================\n",
    "import pandas as pd\n",
    "text_df = pd.read_json(\"/content/sample_data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ec39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Help Func\n",
    "# ========================\n",
    "import re\n",
    "\n",
    "def remove_options(text):\n",
    "    pattern = re.compile(r'\\nOptions:\\n- No\\n- Yes')\n",
    "    cleaned_text = pattern.sub('', text)\n",
    "    pattern = re.compile(r'\\nOptions:\\n- Yes\\n- No')\n",
    "    cleaned_text = pattern.sub('', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def split_A(text):\n",
    "    # 使用 '\\n\\n' 分割文本成多个片段\n",
    "    # segments = text.split('\\n\\n')\n",
    "    pattern = re.compile(r'(yes|no)\\s*\\n\\n', re.IGNORECASE)\n",
    "    segments = pattern.split(text)\n",
    "\n",
    "    valid_segments = []\n",
    "    buff = []\n",
    "    for segment in segments:\n",
    "      buff.append(segment)\n",
    "      if ( len(buff)==2 and\n",
    "        segment.lower() in {'yes', 'no'}):\n",
    "        valid_segments.append(buff )\n",
    "        buff = [ ]\n",
    "      if ( len(buff)==2 and\n",
    "        segment.lower() not in {'yes', 'no'}):\n",
    "        buff = [ ]\n",
    "\n",
    "\n",
    "\n",
    "    # for segment in segments:\n",
    "    #     # 使用空白字符分割片段，分离出前面的部分和最后一个单词\n",
    "    #     parts = segment.rsplit(maxsplit=1)\n",
    "\n",
    "    #     if len(parts) == 2:\n",
    "    #         body, last_word = parts\n",
    "    #         # 判断最后一个单词是否为小写的 'yes' 或 'no'\n",
    "    #         if last_word.lower() in {'yes', 'no'}:\n",
    "    #             valid_segments.append(parts)\n",
    "\n",
    "    return valid_segments\n",
    "def find_puncidx_before_does(text):\n",
    "    pattern = re.compile(r'([\\n:\"])(?=\\s*Does\\s)')#, re.IGNORECASE\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return match.start()\n",
    "    else:\n",
    "        return None\n",
    "def process_text(text):\n",
    "\n",
    "  L_HQ_A = split_A(\n",
    "      remove_options(text)\n",
    "  )\n",
    "  res = []\n",
    "  for HQ,A in L_HQ_A:\n",
    "    idx = find_puncidx_before_does(HQ)\n",
    "    if idx is None:\n",
    "      res.append( (\"[HQA]\",HQ,A) )\n",
    "    else:\n",
    "      res.append( (HQ[:idx+1],HQ[idx+1:].replace(\"\\n\",\"\"),A) )\n",
    "  return res\n",
    "  # idx = find_puncidx_before_does(HQ)\n",
    "  # H,Q = HQ[:idx+1],HQ[idx+1:]\n",
    "  # return \"[HEAD]\"+H,'[QUE]'+Q,'[ASW]'+A\n",
    "def findHQA(vsl):\n",
    " return any([True if v[0]=='[HQA]' else False for v in  vsl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b278c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ========================\n",
    "# Process\n",
    "# ========================\n",
    "\n",
    "text_df['resList2'] = text_df['input'].map(process_text )\n",
    "text_df['processERR2'] = text_df['resList2'].map( findHQA )\n",
    "\n",
    "print(\n",
    "    \"解析失败句子对个数\",\n",
    "    sum([len(x) for x in text_df[text_df['processERR2']==True]['resList2'].values])\n",
    "  )\n",
    "\n",
    "print(\n",
    "    '解析成功句子对个数:',\n",
    "    sum([len(x) for x in text_df[text_df['processERR2']==False]['resList2'].values])\n",
    ")\n",
    "\n",
    "# 解析失败句子对个数 0\n",
    "\n",
    "# 解析成功句子对个数: 102735\n",
    "\n",
    "# CPU times: user 1.55 s, sys: 0 ns, total: 1.55 s\n",
    "\n",
    "# Wall time: 1.55 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1219d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Saving\n",
    "# ========================\n",
    "\n",
    "jsondata = []\n",
    "for id1,vls in zip(text_df['id'].values,text_df['resList2'].values):\n",
    "  for i,vl in enumerate(vls):\n",
    "    idv = f'{id1}-{i}'\n",
    "    jsondata.append(\n",
    "        {\"id\": idv,\n",
    "        \"head\":vl[0],\n",
    "        \"Question\": vl[1],\n",
    "        \"Answer\": vl[2]\n",
    "  })\n",
    "\n",
    "import json\n",
    "with open('/content/sample_data/AdaptLLM-finance-tasks-Headline.json', 'w') as f:\n",
    "    json.dump(jsondata, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
